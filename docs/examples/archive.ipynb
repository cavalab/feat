{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Feat's archive\n",
    "\n",
    "Feat optimizes a population of models. \n",
    "At the end of the run, it can be useful to explore this population to find a trade-off between objectives, \n",
    "such as performance and complexity. \n",
    "\n",
    "In this example, we apply Feat to a regression problem and visualize the archive of representations. \n",
    "\n",
    "Note: this code uses the Penn ML Benchmark Suite (https://github.com/EpistasisLab/penn-ml-benchmarks/) to fetch data. You can install it using `pip install pmlb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the data and create a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmlb import fetch_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import numpy as np\n",
    "# fix the random state\n",
    "random_state=42\n",
    "dataset='690_visualizing_galaxy'\n",
    "X, y = fetch_data(dataset,return_X_y=True)\n",
    "X_t,X_v, y_t, y_v = train_test_split(X,y,train_size=0.75,test_size=0.25,random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we set up a Feat instance and train the model, storing the final archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feat import Feat\n",
    "\n",
    "\n",
    "fest = Feat(pop_size=500, # population size\n",
    "            gens=100, # maximum generations                            \n",
    "            max_time=60, # max time in seconds \n",
    "            max_depth=2, # constrain features depth                                                      \n",
    "            max_dim=5, # constrain representation dimensionality                                                      \n",
    "            random_state=random_state,                                                            \n",
    "            hillclimb=True, # use stochastic hillclimbing to optimize weights                                                   \n",
    "            iters=10, # iterations of hillclimbing\n",
    "            n_jobs=1, # restricts to single thread                                                      \n",
    "            verbosity=2, # verbose output (this will go to terminal, sry..)                                                      \n",
    "           ) \n",
    "\n",
    "print('FEAT version:', fest.__version__)\n",
    "# train the model\n",
    "fest.fit(X_t,y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test score\n",
    "test_score = {}\n",
    "test_score['feat'] = mse(y_v,fest.predict(X_v))\n",
    "\n",
    "# store the archive\n",
    "archive = fest.get_archive(justfront=True)\n",
    "\n",
    "# print the archive\n",
    "print('complexity','fitness','validation fitness',\n",
    "     'eqn')\n",
    "order = np.argsort([a['complexity'] for a in archive])\n",
    "complexity = []\n",
    "fit_train = []\n",
    "fit_test = []\n",
    "eqn = []\n",
    "\n",
    "for o in order:\n",
    "    model = archive[o]\n",
    "    if model['rank'] == 1:\n",
    "        print(model['complexity'],\n",
    "              model['fitness'],\n",
    "              model['fitness_v'],\n",
    "              model['eqn'],\n",
    "             )\n",
    "\n",
    "        complexity.append(model['complexity'])\n",
    "        fit_train.append(model['fitness'])\n",
    "        fit_test.append(model['fitness_v'])\n",
    "        eqn.append(model['eqn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we can fit an Elastic Net and Random Forest regression model to the same data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=random_state)\n",
    "\n",
    "rf.fit(X_t,y_t)\n",
    "\n",
    "test_score['rf'] = mse(y_v,rf.predict(X_v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "linest = ElasticNet()\n",
    "\n",
    "linest.fit(X_t,y_t)\n",
    "\n",
    "test_score['elasticnet'] = mse(y_v,linest.predict(X_v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the test set mean squared errors by method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Archive\n",
    "\n",
    "Let's visualize this archive with the test scores. This gives us a sense of how increasing the representation\n",
    "complexity affects the quality of the model and its generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "%matplotlib inline \n",
    "sns.set_style('white')\n",
    "h = plt.figure(figsize=(14,8))\n",
    "\n",
    "# plot archive points \n",
    "plt.plot(fit_train,complexity,'--ro',label='Train',markersize=6)\n",
    "plt.plot(fit_test,complexity,'--bx',label='Validation')\n",
    "# some models to point out\n",
    "best = np.argmin(np.array(fit_test))\n",
    "middle = np.argmin(np.abs(np.array(fit_test[:best])-test_score['rf']))\n",
    "small = np.argmin(np.abs(np.array(fit_test[:middle])-test_score['elasticnet']))\n",
    "\n",
    "print('best:',complexity[best])\n",
    "print('middle:',complexity[middle])\n",
    "print('small:',complexity[small])\n",
    "plt.plot(fit_test[best],complexity[best],'sk',markersize=16,markerfacecolor='none',label='Model Selection')\n",
    "\n",
    "# test score lines\n",
    "y1 = -1\n",
    "y2 = np.max(complexity)+1\n",
    "plt.plot((test_score['feat'],test_score['feat']),(y1,y2),'--k',label='FEAT Test',alpha=0.5)\n",
    "plt.plot((test_score['rf'],test_score['rf']),(y1,y2),'-.xg',label='RF Test',alpha=0.5)\n",
    "plt.plot((test_score['elasticnet'],test_score['elasticnet']),(y1,y2),'-sm',label='ElasticNet Test',alpha=0.5)\n",
    "\n",
    "print('complexity',complexity)\n",
    "xoff = 100\n",
    "for e,t,c in zip(eqn,fit_test,complexity):\n",
    "    if c in [complexity[best],complexity[middle],complexity[small]]:\n",
    "        t = t+xoff\n",
    "        tax = plt.text(t,c,'$\\leftarrow'+e+'$',size=18,horizontalalignment='left',\n",
    "                      verticalalignment='center')\n",
    "        tax.set_bbox(dict(facecolor='white', alpha=0.75, edgecolor='k'))\n",
    "\n",
    "l = plt.legend(prop={'size': 16},loc=[1.01,0.25])\n",
    "plt.xlabel('MSE',size=16)\n",
    "plt.xlim(np.min(fit_train)*.75,np.max(fit_test)*2)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "\n",
    "plt.gca().set_yticklabels('')\n",
    "plt.gca().set_xticklabels('')\n",
    "\n",
    "plt.ylabel('Complexity',size=18)\n",
    "h.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ElasticNet produces a similar test score to the linear representation\n",
    "in Feat's archive, and that Random Forest's test score is near the representation shown in the middle.\n",
    "\n",
    "The best model, marked with a square, is selected from the validation curve (blue line).\n",
    "The validation curve shows how models begin to overfit as complexity grows.\n",
    "By visualizing the archive, we can see that some lower complexity models achieve nearly as good of a validation score.\n",
    "In this case it may be preferable to choose that representation instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, FEAT will choose the model with the lowest validation error, marked with a square above. \n",
    "Let's look at that model.\n",
    "\n",
    "the function `get_model()` will print a table of the learned features, optionally ordered by the magnitude of their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fest.get_model(sort=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
