{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation: a backpropagation example for genetic programming\n",
    "\n",
    "The purpose of this notebook is to demonstrate how weights can be learned for programs made up of differentiable nodes. In this case we're concerned with stack-based representations of mathematical functions, such as \n",
    "\n",
    "[ x1 x2 + sin ] $\\rightarrow sin(x1+x2)$\n",
    "\n",
    "or with embedded weights, $w_4 \\cdot \\sin( w_3 \\cdot ( w_1 \\cdot x_1 + w_2 \\cdot x_2) ) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define node classes\n",
    "# Deal with elements that aren't differentiable (keep executing branch but skip past that value)\n",
    "class Variable():\n",
    "    def __init__(self,loc=0):\n",
    "        self.loc = loc\n",
    "        self.name='x_'+str(loc)\n",
    "        self.arity=0\n",
    "        self.visits=0\n",
    "                \n",
    "    def evaluate(self,stack,X=None):\n",
    "        stack.append(X[:,self.loc])\n",
    "    \n",
    "class Add():\n",
    "    def __init__(self, W=None):\n",
    "        self.name='+'\n",
    "        self.arity=2\n",
    "        self.visits=0\n",
    "        \n",
    "        # Technically have vulnerability to lists of non numbers\n",
    "        if isinstance(W, list) and len(W) == self.arity: self.W = W\n",
    "        else: self.W = [1.0, 1.0]\n",
    "            \n",
    "    def evaluate(self,stack,X=None):\n",
    "        stack.append(self.W[0]*stack.pop() + self.W[1]*stack.pop())\n",
    "        \n",
    "    def derivative(self,stack,fwd_stack,loc=0):\n",
    "        if loc > self.arity: loc = 0\n",
    "            \n",
    "        if loc == 0: stack.append(self.W[0])\n",
    "        else: stack.append(self.W[1])\n",
    "        \n",
    "    def update(self,derivatives,fwd_stack,n):\n",
    "        update_value = 1\n",
    "        for d in derivatives[:]: # Calculate the chain rule\n",
    "            update_value *= d\n",
    "        \n",
    "        W = self.W[:] # don't want weight update to cascade and change gradient for other weights\n",
    "        d_w = fwd_stack[-1]\n",
    "        self.W[0] = W[0] - n/update_value.size * sum(d_w * update_value)\n",
    "        value = W[1] - n/update_value.size * sum(d_w * update_value)\n",
    "        # print(\"Add updating W0 with\", value, ' to value: ', self.W[0])\n",
    "        d_w = fwd_stack[-2]\n",
    "        self.W[1] = W[1] - n/update_value.size * sum(d_w * update_value)\n",
    "        value = n/update_value.size * sum(d_w * update_value)\n",
    "        # print(\"Add updating W1 with\", value, ' to value: ', self.W[1])\n",
    "        \n",
    "        \n",
    "class Subtract():\n",
    "    def __init__(self,W=None):\n",
    "        self.name='-'\n",
    "        self.arity=2\n",
    "        self.visits=0\n",
    "        if isinstance(W, list) and len(W) == self.arity: self.W = W\n",
    "        else: self.W = [1.0, 1.0]\n",
    "        \n",
    "    def evaluate(self,stack,X=None):\n",
    "        stack.append(self.W[0]*stack.pop() - self.W[1]*stack.pop())\n",
    "    \n",
    "    def derivative(self,stack,fwd_stack,loc=0):\n",
    "        if loc > self.arity: loc = 0\n",
    "            \n",
    "        if loc == 0: stack.append(self.W[0])\n",
    "        else: stack.append(- self.W[1])\n",
    "        \n",
    "    def update(self,derivatives,fwd_stack,n):\n",
    "        update_value = 1\n",
    "        for d in derivatives:\n",
    "            update_value *= d\n",
    "            \n",
    "        W = self.W[:]\n",
    "        d_w = fwd_stack[-1]\n",
    "        self.W[0] = W[0] - n/update_value.size * sum(d_w * update_value)\n",
    "        value = W[1] - n/update_value.size * sum(d_w * update_value)\n",
    "        # print(\"Subtract updating W0 with\", value, ' to value: ', self.W[0])\n",
    "        d_w = - fwd_stack[-2]\n",
    "        self.W[1] = W[1] - n/update_value.size * sum(d_w * update_value)\n",
    "        value = n/update_value.size * sum(d_w * update_value)\n",
    "        # print(\"Subtract updating W1 with\", value, ' to value: ', self.W[1])\n",
    "        \n",
    "class Multiply():\n",
    "    def __init__(self,W=None):\n",
    "        self.name='*'\n",
    "        self.arity=2\n",
    "        self.error=0\n",
    "        self.visits=0\n",
    "        \n",
    "        if isinstance(W, list) and len(W) == self.arity - 1: self.W = W\n",
    "        else: self.W = [1.0, 1.0]\n",
    "            \n",
    "    def evaluate(self,stack,X=None):\n",
    "        stack.append(self.W[0] * stack.pop() * self.W[1] * stack.pop())\n",
    "    \n",
    "    def derivative(self,stack,fwd_stack,loc=0):\n",
    "        if loc > self.arity: loc = 0\n",
    "        \n",
    "        if loc == 0:\n",
    "            stack.append(self.W[0] * self.W[1] * fwd_stack[-2]) \n",
    "        elif loc == 1:\n",
    "            stack.append(self.W[0] * self.W[1] * fwd_stack[-1]) \n",
    "        \n",
    "    def update(self,derivatives,fwd_stack,n):\n",
    "        update_value = 1\n",
    "        for d in derivatives:\n",
    "            update_value *= d\n",
    "        \n",
    "        W = self.W[:]\n",
    "        d_w = fwd_stack[-1] * W[1] * fwd_stack[-2]\n",
    "        self.W[0] = W[0] - n/update_value.size * sum(d_w * update_value)\n",
    "        d_w = fwd_stack[-1] * W[0] * fwd_stack[-2]\n",
    "        self.W[1] = W[1] - n/update_value.size * sum(d_w * update_value)\n",
    "        # print(\"Mult updating W1 to \", self.W[1])\n",
    "\n",
    "class Divide():\n",
    "    def __init__(self,W=None):\n",
    "        self.name='/'\n",
    "        self.arity=2\n",
    "        self.error=0\n",
    "        self.visits=0\n",
    "        if isinstance(W, list) and len(W) == self.arity: self.W = W\n",
    "        else: self.W = [1.0, 1.0]\n",
    " \n",
    "    def evaluate(self,stack,X=None):\n",
    "        x2 = stack.pop()\n",
    "        x1 = stack.pop()\n",
    "        stack.append(self.W[1] * x1/(self.W[0] * x2))\n",
    "    \n",
    "    def derivative(self,stack,fwd_stack,loc=0):\n",
    "        if loc > self.arity: loc = 0\n",
    "        \n",
    "        if loc==0: stack.append(self.W[1]/(self.W[0] * fwd_stack[-1]))\n",
    "        else: stack.append(-self.W[1] * fwd_stack[-2]/(self.W[0] * fwd_stack[-1]**2))\n",
    "        \n",
    "    def update(self,derivatives,fwd_stack,n):            \n",
    "        update_value = 1\n",
    "        for d in derivatives:\n",
    "            update_value *= d\n",
    "            print(\"Gradient: \", d)\n",
    "        x2 = fwd_stack[-1]\n",
    "        x1 = fwd_stack[-2]\n",
    "        W = self.W[:]\n",
    "        d_w = -W[1] * x1/(x2 * W[0]**2)\n",
    "        self.W[0] = W[0] - n/update_value.size * sum(d_w * update_value)\n",
    "        print(\"Divide updating W1 with\", d_w, ' to: ', self.W[0])\n",
    "        d_w = x1/(W[0] * x2)\n",
    "        self.W[1] = W[1] - n/update_value.size * sum(d_w * update_value)\n",
    "        print(\"Divide updating W2 with\", d_w, ' to: ', self.W[1])\n",
    " \n",
    "class Sin():\n",
    "    def __init__(self,W=None):\n",
    "        self.name='sin'\n",
    "        self.arity=1\n",
    "        self.error=0\n",
    "        self.visits=0\n",
    "        if isinstance(W, list) and len(W) == self.arity: self.W = W\n",
    "        else: self.W = [1.0]\n",
    "        \n",
    "    def evaluate(self,stack,X=None):\n",
    "        stack.append(np.sin(self.W[0] * stack.pop()))\n",
    "    \n",
    "    def derivative(self,stack,fwd_stack,loc=0):\n",
    "        if loc > self.arity: loc = 0\n",
    "        stack.append(self.W[loc] * np.cos(self.W[loc] * fwd_stack[-1]))\n",
    "        \n",
    "    def update(self,derivatives,fwd_stack,n,loc=-1):\n",
    "        update_value = 1\n",
    "        for d in derivatives[:]:\n",
    "            update_value *= d\n",
    "            # print(\"Updating with \", d)\n",
    "        d_w = fwd_stack[-1] * np.cos(self.W[loc] * fwd_stack[-1])\n",
    "        self.W[loc] = self.W[loc] - n/update_value.size * sum(d_w * update_value)\n",
    "        value = n/update_value.size * sum(d_w * update_value)\n",
    "        print(\"Sin updating W with\", value, ' to: ', self.W[0])\n",
    "        \n",
    "class Cos():\n",
    "    def __init__(self,W=None):\n",
    "        self.name='cos'\n",
    "        self.arity=1\n",
    "        self.error=0\n",
    "        self.visits=0\n",
    "        if isinstance(W, list) and len(W) == self.arity: self.W = W\n",
    "        else: self.W = [1.0]\n",
    "            \n",
    "    def evaluate(self,stack,X=None):\n",
    "        stack.append(np.cos(self.W[0] * stack.pop()))\n",
    "    \n",
    "    def derivative(self,stack,fwd_stack,loc=0):\n",
    "        if loc > self.arity: loc = 0\n",
    "        stack.append(self.W[loc] * -np.sin(self.W[loc] * fwd_stack[-1])) \n",
    "        \n",
    "    def update(self,derivatives,fwd_stack,n,loc=0):\n",
    "        print(\"------------------------------------\")\n",
    "        update_value = 1\n",
    "        for d in derivatives[:]:\n",
    "            update_value *= d\n",
    "            print(\"D: \", d)\n",
    "        \n",
    "        #print(\"Update value: \", update_value)\n",
    "        d_w = fwd_stack[-1] * -np.sin(self.W[0] * fwd_stack[-1])\n",
    "        print(\"Derivative: \", d_w)\n",
    "        print(fwd_stack[-1])\n",
    "        self.W[loc] = self.W[loc] - n/update_value.size * sum(d_w * update_value)\n",
    "        value = n/update_value.size * sum(d_w * update_value)\n",
    "        print(\"Cos update value: \", sum(d_w * update_value))\n",
    "        print(\"Cos updating W with\", value, ' to value: ', self.W[0])\n",
    "        \n",
    "class Logit():\n",
    "    def __init__(self,W=None):\n",
    "        self.name='logit'\n",
    "        self.arity=1\n",
    "        self.visits=0\n",
    "        if isinstance(W, list) and len(W) == self.arity: self.W = W\n",
    "        else: self.W = [1.0]\n",
    "            \n",
    "    def evaluate(self,stack,X=None):\n",
    "        stack.append(1/(1 + np.exp(-self.W[0] * stack.pop())))\n",
    "    \n",
    "    def derivative(self,stack,fwd_stack,loc=0):\n",
    "        if loc > self.arity: loc = 0\n",
    "            \n",
    "        numerator = self.W[0] * np.exp(-self.W[0] * fwd_stack[-1])\n",
    "        denom = (1 + np.exp(-self.W[0] * fwd_stack[-1]))**2\n",
    "        stack.append(numerator/denom)\n",
    "        \n",
    "    def update(self,derivatives,fwd_stack,n):\n",
    "        update_value = 1\n",
    "        for d in derivatives:\n",
    "            update_value *= d\n",
    "                 \n",
    "        numerator = fwd_stack[-1] * np.exp(-self.W[0] * fwd_stack[-1])\n",
    "        denom = (1 + np.exp(-self.W[0] * fwd_stack[-1]))**2\n",
    "        d_w = numerator/denom\n",
    "        self.W[0] = self.W[0] - n/update_value.size * sum(d_w * update_value)\n",
    "        value = n/update_value.size * sum(d_w * update_value)\n",
    "        # print(\"Logit updating W0 with\", (n/update_value.size * sum(d_w * update_value)), ' to value: ', self.W[0])\n",
    "      \n",
    "# How do we deal with multiple outputs in the context of the program?\n",
    "class Softmax2(): # Vector operating node - takes in 2 inputs and outputs wrt 1st\n",
    "    def __init__(self,W=None):\n",
    "        self.name='softmax'\n",
    "        self.arity=2\n",
    "        self.visits=0\n",
    "        if isinstance(W, list) and len(W) == self.arity: self.W = W\n",
    "        else: self.W = [1.0 for x in range(self.arity)]\n",
    "            \n",
    "    def evaluate(self,stack,X=None):\n",
    "        values = [np.exp(self.W[i] * stack.pop()) for i in range(self.arity)]        \n",
    "        stack.append(values[1]/sum(values))\n",
    "    \n",
    "    def derivative(self,stack,fwd_stack,loc=0):\n",
    "        if loc > self.arity: loc = 0\n",
    "        \n",
    "        if loc == 0:\n",
    "            numerator = self.W[0] * np.exp(self.W[0] * fwd_stack[-1] + self.W[1] * fwd_stack[-2])\n",
    "            denom = (np.exp(self.W[0] * fwd_stack[-1]) + np.exp(self.W[1] * fwd_stack[-2]))**2\n",
    "            stack.append(numerator/denom)\n",
    "        elif loc == 1:\n",
    "            numerator = self.W[1] * np.exp(self.W[0] * fwd_stack[-1] + self.W[1] * fwd_stack[-2])\n",
    "            denom = (np.exp(self.W[0] * fwd_stack[-1]) + np.exp(self.W[1] * fwd_stack[-2]))**2\n",
    "            stack.append(numerator/denom)\n",
    "        \n",
    "    def update(self,derivatives,fwd_stack,n):\n",
    "        update_value = 1\n",
    "        for d in derivatives:\n",
    "            update_value *= d\n",
    "        \n",
    "        W = self.W[:]\n",
    "        numerator = fwd_stack[-1] * np.exp(W[0] * fwd_stack[-1] + W[1] * fwd_stack[-2])\n",
    "        denom = (np.exp(W[0] * fwd_stack[-1]) + np.exp(W[1] * fwd_stack[-2]))**2\n",
    "        d_w = numerator/denom\n",
    "        self.W[0] = W[0] - n/update_value.size * sum(d_w * update_value)\n",
    "        numerator = W[1] * np.exp(W[0] * fwd_stack[-1] + W[1] * fwd_stack[-2])\n",
    "        denom = (np.exp(W[0] * fwd_stack[-1]) + np.exp(W[1] * fwd_stack[-2]))**2\n",
    "        d_w = numerator/denom\n",
    "        self.W[1] = W[1] - n/update_value.size * sum(d_w * update_value)\n",
    "        \n",
    "class Tanh():\n",
    "    def __init__(self,W=None):\n",
    "        self.name='tanh'\n",
    "        self.arity=1\n",
    "        self.visits=0\n",
    "        if isinstance(W, list) and len(W) == self.arity: self.W = W\n",
    "        else: self.W = [1.0]\n",
    "            \n",
    "    def evaluate(self,stack,X=None):\n",
    "        z = self.W[0] * stack.pop()\n",
    "        stack.append((np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z)))\n",
    "    \n",
    "    def derivative(self,stack,fwd_stack,loc=0):\n",
    "        if loc > self.arity: loc = 0\n",
    "        \n",
    "        numerator = 4 * self.W[0] * np.exp(2 * self.W[0] * fwd_stack[-1])\n",
    "        denom = (np.exp(2 * self.W[0] * fwd_stack[-1]) + 1)**2\n",
    "        stack.append(numerator/denom) # Might be an error from this statement\n",
    "        # stack.append(self.W[0] * (1/np.arccosh(self.W[0] * fwd_stack[-1]))**2)\n",
    "        \n",
    "    def update(self,derivatives,fwd_stack,n):\n",
    "        update_value = 1\n",
    "        for d in derivatives:\n",
    "            update_value *= d\n",
    "        \n",
    "        numerator = 4 * fwd_stack[-1] * np.exp(2 * self.W[0] * fwd_stack[-1])\n",
    "        denom = (np.exp(2 * self.W[0] * fwd_stack[-1]) + 1)**2\n",
    "        d_w = numerator/denom\n",
    "        \n",
    "        self.W[0] = self.W[0] - n/update_value.size * sum(d_w * update_value)\n",
    "       #  print(\"Tanh updating to: \", self.W[0])\n",
    "        \n",
    "class Bias():\n",
    "    def __init__(self,W=None):\n",
    "        self.name='tanh'\n",
    "        self.arity=1\n",
    "        self.visits=0\n",
    "        if isinstance(W, list) and len(W) == self.arity: self.W = W\n",
    "        else: self.W = [1.0]\n",
    "            \n",
    "    def evaluate(self,stack,X=None):\n",
    "        stack.append(self.W)\n",
    "    \n",
    "    def derivative(self,stack,fwd_stack,loc=0):\n",
    "        stack.append(1);\n",
    "        \n",
    "    def update(self,derivatives,fwd_stack,n):\n",
    "        update_value = 1\n",
    "        for d in derivatives:\n",
    "            update_value *= d\n",
    "        \n",
    "        self.W[0] = self.W[0] - n/update_value.size * sum(update_value)\n",
    "        print(\"Bias updating to: \", self.W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Functions\n",
    "class CostSquaredDifference():\n",
    "    def __init__(self,labels=None):\n",
    "        self.visits=0\n",
    "        self.arity=2\n",
    "        self.name=\"Squared Error Cost\"\n",
    "        try: \n",
    "            _ = labels.shape\n",
    "            self.arity=1\n",
    "            self.labels=labels\n",
    "        except:\n",
    "            self.labels=labels\n",
    "        \n",
    "    def set_labels(self, labels):\n",
    "        try: \n",
    "            _ = labels.shape\n",
    "            self.arity=1\n",
    "        except:\n",
    "            self.labels=None\n",
    "            self.arity=2\n",
    "    \n",
    "    def evaluate(self,Y_hat,Y=None,loc=0):\n",
    "        try: \n",
    "            result = (Y_hat - Y)**2\n",
    "        except:\n",
    "            result = (Y_hat - self.labels)**2\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def derivative(self,Y_hat,Y=None):\n",
    "        print(\"Cost function derivative pred: \", Y_hat)\n",
    "        try: \n",
    "            print(\"Cost function derivative label: \", Y)\n",
    "            result = 2*(Y_hat - Y)\n",
    "        except:\n",
    "            print(\"Cost function derivative label: \", self.labels)\n",
    "            result = 2*(Y_hat - self.labels)\n",
    "        return result\n",
    "    \n",
    "    def update(self,derivative,fwd_stack,n,loc):\n",
    "        pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Auto_backprop():\n",
    "    def __init__(self, program, cost_fcn, X, labels, iters=1000, n=0.1):\n",
    "        self.program = program    # Program to modify (stack)\n",
    "        self.cost_fcn = cost_fcn  # Determines how error is calculated (expected args are y_hat, y)\n",
    "        self.X = X                # Training data (list of values)\n",
    "        self.labels = labels      # Training labels (list of values)\n",
    "        self.iters = iters        # Iterations of learning (scalar)\n",
    "        self.n = n                # Learning Rate (scalar)\n",
    "        \n",
    "    def start(self):\n",
    "        # \"Intialize\" cost function\n",
    "        self.cost_fcn.set_labels(self.labels)\n",
    "        \n",
    "        # Computes weights via backprop\n",
    "        for x in range(self.iters):            \n",
    "            fwd_stack = self.forward_prop()\n",
    "#             if (x % round(self.iters/4)) == 0 or x == self.iters - 1:\n",
    "#                 print(\"Currently on iter: \" + str(x))\n",
    "#                 print(\"Error: \" + str(np.mean(self.cost_fcn.evaluate(fwd_stack[-1]))))\n",
    "            self.backprop(fwd_stack)\n",
    "            \n",
    "        print(\"Gradient Descent Complete ------------------------------\")\n",
    "        return self.program\n",
    "        \n",
    "    def forward_prop(self):\n",
    "        # Computes the forward pass of the program as described by the stack\n",
    "        execution_stack = []\n",
    "        fwd_stack = []\n",
    "        \n",
    "        # Program stack itself is unchanging\n",
    "        for p in self.program:\n",
    "            p.visits=0 # Clear our values set by backprop\n",
    "            \n",
    "            # Keep track of what computations were needed for execution (for purpose of GD)\n",
    "            for x in range(p.arity):\n",
    "                fwd_stack.append(execution_stack[-(p.arity - x)])\n",
    "                \n",
    "            p.evaluate(execution_stack, self.X)\n",
    "\n",
    "        # Add the final prediction to the stack\n",
    "        fwd_stack.append(execution_stack.pop())\n",
    "        \n",
    "        return fwd_stack\n",
    "    \n",
    "    # Returns the node and derivative of the parent, returns None if not found\n",
    "    def get_next_branch(self, executing, bp_program, gradients):\n",
    "        # Every node should have an associated gradient\n",
    "        if executing:\n",
    "            n_derivatives = []\n",
    "            while not n_derivatives:\n",
    "                (node, n_derivatives) = executing.pop()\n",
    "                gradients.pop()\n",
    "                \n",
    "                if not executing:\n",
    "                    return None\n",
    "            \n",
    "            # Should now have the next parent node\n",
    "            bp_program.append(node)\n",
    "            gradients.append(n_derivatives.pop(0))\n",
    "            executing.append((node, n_derivatives))\n",
    "        return None\n",
    "        \n",
    "    def backprop(self, fwd_stack):\n",
    "        # Prepare stacks\n",
    "        gradients = [self.cost_fcn.derivative(fwd_stack[-1])]\n",
    "        print(\"Gradients: \", gradients)\n",
    "        fwd_stack.pop() # Value is not an input to any function, not needed for derivatives\n",
    "        \n",
    "        executing = []\n",
    "        bp_program = copy.copy(self.program) # Using a counter approach would allow avoiding this\n",
    "        \n",
    "        while bp_program:\n",
    "            node = bp_program.pop() \n",
    "            # print(\"Evaluating: \", node.name)\n",
    "            n_derivatives = []\n",
    "            if node.visits == 0 and node.arity > 0:\n",
    "                # New implementation: Calculate all gradients and updates at same time\n",
    "                # If have arity > 1 then store extra derivatives on executing stack in following manner:\n",
    "                # Stack holds elements with form: (node, [list of derivatives])\n",
    "                # Once all derivatives are calculated can throw input values away\n",
    "                \n",
    "                for i in range(node.arity):\n",
    "                    node.derivative(n_derivatives, fwd_stack, node.arity - 1 - i) # Values need to be calculated before updates\n",
    "                    \n",
    "                node.update(gradients,fwd_stack,self.n) # Update all weights\n",
    "                \n",
    "                # Get rid of all input arguments\n",
    "                for x in range(node.arity):\n",
    "                    fwd_stack.pop()\n",
    "                \n",
    "                # Add the gradient for the first branch into the current executing gradients\n",
    "                if n_derivatives:\n",
    "                    gradients.append(n_derivatives.pop(0))\n",
    "                # Keep track of all gradients currently on stack\n",
    "                executing.append((node, n_derivatives))\n",
    "            \n",
    "            # Care about tree structure for backpropping gradients\n",
    "            if node.arity == 0:\n",
    "                # Clean up gradients and find the parent node\n",
    "                self.get_next_branch(executing, bp_program, gradients)\n",
    "            else:\n",
    "                # Must be an operator\n",
    "                node.visits += 1 # Consider moving to after\n",
    "                \n",
    "                if node.visits > node.arity:         \n",
    "                    self.get_next_branch(executing, bp_program, gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error before GD: 63.53349929019109\n",
      "('Cost function derivative pred: ', array([0.45869332, 0.64384076]))\n",
      "('Cost function derivative label: ', None)\n",
      "('Cost function derivative label: ', array([9., 8.]))\n",
      "('Gradients: ', [array([-17.08261337, -14.71231847])])\n",
      "('Sin updating W with', -4.38782737833633, ' to: ', 5.38782737833633)\n",
      "------------------------------------\n",
      "('D: ', array([-17.08261337, -14.71231847]))\n",
      "('Derivative: ', array([-6.20818733, -2.71249447]))\n",
      "[7.3 6.7]\n",
      "('Cos update value: ', 145.9591463604284)\n",
      "('Cos updating W with', 7.297957318021421, ' to value: ', -6.297957318021421)\n",
      "Gradient Descent Complete ------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# X = np.random.rand(100,2)\n",
    "data1 = np.asarray([1.0 * i for i in range(100)])\n",
    "data2 = np.flip(data1, 0)\n",
    "data1 = np.asarray([7.3, 6.7])\n",
    "data2 = np.asarray([12.4, 13.2])\n",
    "X = np.stack((data1, data2), -1)\n",
    "\n",
    "# test weights\n",
    "w1, w2, w3 = 0.35, 0.76, 0.89\n",
    "ytest = np.cos(w1*np.sin(w2*X[:,0] + w3*X[:,1]))\n",
    "ytest = np.asarray([9.0, 8.0])\n",
    "\n",
    "def run_test(prog, Y, iters):\n",
    "    stack = []\n",
    "    for p in prog:\n",
    "        p.evaluate(stack, X)\n",
    "    yhat2 = np.array(stack[-1])\n",
    "    \n",
    "    cost_fcn = CostSquaredDifference(ytest)\n",
    "    error = cost_fcn.evaluate(yhat2)\n",
    "    print('Error before GD: ' + str(np.mean(error)))\n",
    "    \n",
    "#     h1 = plt.figure()\n",
    "#     plt.plot(ytest,yhat2)\n",
    "#     plt.title('Before GD')\n",
    "#     plt.show()\n",
    "    \n",
    "    GD = Auto_backprop(prog, cost_fcn, X, ytest, iters,0.1)\n",
    "    prog = GD.start()\n",
    "    stack = []\n",
    "    for p in prog:\n",
    "        p.evaluate(stack, X)\n",
    "    yhat3 = np.array(stack[-1])\n",
    "    \n",
    "#     h2 = plt.figure()\n",
    "#     plt.plot(ytest,yhat3)\n",
    "#     plt.title('After')\n",
    "#     plt.show()\n",
    "\n",
    "# h0 = plt.figure()\n",
    "# plt.plot(ytest,ytest)\n",
    "# plt.title('Target')\n",
    "# plt.show()\n",
    "\n",
    "# Testing suite\n",
    "prog_set = [[Variable(loc=0), Variable(loc=1), Add(), Sin(), Cos()],\n",
    "            [Variable(loc=0), Cos(), Variable(loc=1), Sin(), Logit()],\n",
    "            [Variable(loc=0), Sin()], [Variable(loc=0), Cos()], \n",
    "            [Variable(loc=0), Logit()], [Variable(loc=0), Tanh()], # 3 - 4\n",
    "            [Variable(loc=0), Variable(loc=1), Softmax2()], [Variable(loc=0), Variable(loc=1), Divide()],\n",
    "            [Variable(loc=0), Variable(loc=1), Multiply()], [Variable(loc=0), Variable(loc=1), Subtract()],\n",
    "            [Variable(loc=0), Variable(loc=1), Add()], # 9\n",
    "            [Variable(loc=0), Variable(loc=1), Add(), Sin()],\n",
    "            [Variable(loc=0), Variable(loc=1), Subtract(), Cos()],\n",
    "            [Variable(loc=0), Variable(loc=1), Multiply(), Logit()],\n",
    "            [Variable(loc=0), Variable(loc=1), Add(), Tanh()],\n",
    "            [Variable(loc=0), Cos(), Variable(loc=1), Sin(), Add(), Logit()],\n",
    "            [Variable(loc=0), Cos(), Tanh(), Sin(), Variable(loc=1), Sin(), Add(), Logit()],\n",
    "           ]\n",
    "\n",
    "# Attempt backpropagation\n",
    "ytest = np.cos(w1*np.sin(w2*X[:,0] + w3*X[:,1]))\n",
    "ytest = np.asarray([9.0, 8.0])\n",
    "prog = prog_set[15]\n",
    "\n",
    "run_test(prog_set[1],ytest,1) # [Variable(loc=0), Cos(), Tanh(), Sin(), Variable(loc=1), Sin(), Add(), Logit()]\n",
    "#run_test(prog_set[-2],ytest,1000) # [Variable(loc=0), Cos(), Variable(loc=1), Sin(), Add(), Logit()]\n",
    "#run_test(prog_set[-4],ytest,1000) # [Variable(loc=0), Variable(loc=1), Multiply(), Logit()]\n",
    "#run_test(prog_set[0],ytest,1000)  # [Variable(loc=0), Variable(loc=1), Add(), Sin(), Cos()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
